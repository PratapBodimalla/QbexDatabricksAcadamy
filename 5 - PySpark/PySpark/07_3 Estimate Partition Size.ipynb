{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23f25da-899b-4936-a63a-24404b6fcef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ba03929a6386:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Factor of cores</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fae10187c70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Factor of cores\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"1G\") \\\n",
    "    .config(\"spark.driver.memory\", \"4G\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a351371d-618c-4237-8f82-a48ad2289010",
   "metadata": {},
   "outputs": [],
   "source": [
    "executor_memory = spark.conf.get(\"spark.executor.memory\", \"Not set (local mode uses driver memory)\")\n",
    "driver_memory = spark.conf.get(\"spark.driver.memory\", \"Default (e.g., 1G)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd8fee0-e6c2-4c06-bb04-a634d3e6e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "executor_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2414d6dd-5586-4e80-a1b9-04ec0368c179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the degree of parallelism\n",
    "spark.sparkContext.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a5712f9-4b44-404a-ab41-3447e71e6946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'134217728b'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.files.maxPartitionBytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d396f15c-219d-4cf2-b220-63928eff0694",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(70 * 1024 * 1024)+\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93da9724-c559-454f-940d-1267622ccc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data File Size: \n",
      "            2898932284 in bytes \n",
      "            2764.637264251709 in MB\n",
      "            2.6998410783708096 in GB\n"
     ]
    }
   ],
   "source": [
    "# File size that we are going to import\n",
    "import os\n",
    "file_size = os.path.getsize('./Input/sample_data.csv')\n",
    "print(f\"\"\"Data File Size: \n",
    "            {file_size} in bytes \n",
    "            {int(file_size) / 1024 / 1024} in MB\n",
    "            {int(file_size) / 1024 / 1024 / 1024} in GB\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75f1776e-0193-49c0-b69d-932f001dcedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order ID,Order Date,Customer ID,Qty,Price,Amount,Sales Region,Country\n",
      "ORD00001,2024-06-15,C1,128,422,54016,South,Uruguay\n",
      "ORD00002,2021-06-16,C2,198,359,71082,East,China\n",
      "ORD00003,2023-10-08,C5,93,305,28365,North,Bermuda\n",
      "ORD00004,2021-10-07,C4,19,564,10716,North,Greenland\n"
     ]
    }
   ],
   "source": [
    "# Specify the file path\n",
    "file_path = \"./Input/sample_data.csv\"\n",
    "\n",
    "# Number of lines to read (including the header if needed)\n",
    "n = 5\n",
    "\n",
    "# Open the file and read lines\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = [file.readline().strip() for _ in range(n)]\n",
    "\n",
    "# Print the sampled lines\n",
    "for line in lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4346d198-06bf-4cf6-9b76-d185509ff99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a simple Python decorator - {get_time} to get the execution timings\n",
    "# If you dont know about Python decorators - check out : https://www.geeksforgeeks.org/decorators-in-python/\n",
    "import time\n",
    "\n",
    "def get_time(func):\n",
    "    def inner_get_time() -> str:\n",
    "        start_time = time.time()\n",
    "        func()\n",
    "        end_time = time.time()\n",
    "        print(\"-\"*80)\n",
    "        return (f\"Execution time: {(end_time - start_time)*1000} ms\")\n",
    "    print(inner_get_time())\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d1bf6cd-d144-40ed-935f-7fc3a7955827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partition -> 40\n",
      "--------------------------------------------------------------------------------\n",
      "Execution time: 53682.899713516235 ms\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Code for benchmarking\n",
    "from pyspark.sql.functions import count, lit\n",
    "@get_time\n",
    "def x(): \n",
    "    df = spark.read.format(\"csv\").option(\"header\",True).load(\"./Input/sample_data.csv\")\n",
    "    print(f\"Number of Partition -> {df.rdd.getNumPartitions()}\")\n",
    "    df.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea7b574-1607-455d-bb78-7f3ca6c2cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define shuffle partitions which is not Factor of core\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39735e2-23cc-4d3f-ae91-58fca7c1a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(128 * 3 * 1024 * 1024)+\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6289cf99-15f9-4abc-a962-3f48bb2c5942",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(462 * 1024 * 1024)+\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c896cc-1a90-4efd-a044-08562bf78706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
