{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f5bb9c-aaeb-40ce-9b9b-9253dba6e3b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ba03929a6386:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Factor of cores</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb36c01fee0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "from os.path import abspath\n",
    "# warehouse_location points to the default location for managed databases and tables\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Factor of cores\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"1G\") \\\n",
    "    .config(\"spark.driver.memory\", \"4G\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c33c53-21f4-4b50-8d2b-768715184a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORD00001,2024-06-15,C1,128,422,54016,South,Uruguay\n",
      "ORD00002,2021-06-16,C2,198,359,71082,East,China\n",
      "ORD00003,2023-10-08,C5,93,305,28365,North,Bermuda\n",
      "ORD00004,2021-10-07,C4,19,564,10716,North,Greenland\n",
      "ORD00005,2022-10-11,C3,110,384,42240,South,Sri Lanka\n"
     ]
    }
   ],
   "source": [
    "# Specify the file path\n",
    "file_path = \"./Input/Sales_order/part-00000-2482645b-99e0-4e87-ba6c-4fc20fed5d14-c000.csv\"\n",
    "\n",
    "# Number of lines to read (including the header if needed)\n",
    "n = 5\n",
    "\n",
    "# Open the file and read lines\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = [file.readline().strip() for _ in range(n)]\n",
    "\n",
    "# Print the sampled lines\n",
    "for line in lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2cbd1b6-e843-4d1d-8acc-1ca7e6af6a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_schema = \"\"\"\n",
    "                Order_Id string,\n",
    "                Order_Date date,\n",
    "                Customer_ID string,\n",
    "                Qty integer,\n",
    "                Price integer,\n",
    "                Amount integer,\n",
    "                Sales_Region string,\n",
    "                Country string\n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba4c8503-280d-497d-b932-11b61c4813a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partition -> 22\n"
     ]
    }
   ],
   "source": [
    "sales_df = spark.read.format(\"csv\").option(\"header\",True).schema(order_schema).load(\"./Input/Sales_order/\")\n",
    "print(f\"Number of Partition -> {sales_df.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50a5995c-34c3-4e55-aea9-2a542df389bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order_Id: string (nullable = true)\n",
      " |-- Order_Date: date (nullable = true)\n",
      " |-- Customer_ID: string (nullable = true)\n",
      " |-- Qty: integer (nullable = true)\n",
      " |-- Price: integer (nullable = true)\n",
      " |-- Amount: integer (nullable = true)\n",
      " |-- Sales_Region: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "546eaffb-62ba-4004-a7ca-e3c125ddf63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_data = [[\"C1\",\"Pratap\"],\n",
    "                [\"C2\",\"Sruthi\"],\n",
    "                 [\"C3\",\"Nirupama\"],\n",
    "                 [\"C4\",\"Kiyanshitha\"],\n",
    "                 [\"C5\",\"Chand\"] ]\n",
    "\n",
    "customer_schema = \"\"\" Customer_ID string, \n",
    "                    Customer_Name string \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b57f7eb-b89d-41fc-bb58-37c748ffc6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = spark.createDataFrame(data=customer_data, schema=customer_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98a9c415-c999-46c4-8171-694c815a3bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a simple Python decorator - {get_time} to get the execution timings\n",
    "# If you dont know about Python decorators - check out : https://www.geeksforgeeks.org/decorators-in-python/\n",
    "import time\n",
    "\n",
    "def get_time(func):\n",
    "    def inner_get_time() -> str:\n",
    "        start_time = time.time()\n",
    "        func()\n",
    "        end_time = time.time()\n",
    "        print(\"-\"*80)\n",
    "        return (f\"Execution time: {(end_time - start_time)*1000} ms\")\n",
    "    print(inner_get_time())\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d709062-92db-4bf5-95a7-70c7121d291c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbfd45cb-f96a-4072-bec8-9f287f1e0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "700ac3ef-d45c-4572-a8e6-f96f785acd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10485760b'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec643803-2817-4163-998e-922d07200003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Execution time: 230696.02060317993 ms\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Code for benchmarking\n",
    "from pyspark.sql.functions import count, lit \n",
    "@get_time\n",
    "def x(): \n",
    "    df_joined = sales_df.join(customer_df, on=sales_df.Customer_ID==customer_df.Customer_ID, how=\"left_outer\")\n",
    "    df_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31171188-dce4-416d-a01b-3e91a2d24657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Execution time: 113922.34110832214 ms\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Code for benchmarking\n",
    "from pyspark.sql.functions import count, lit , broadcast\n",
    "@get_time\n",
    "def x(): \n",
    "    df_joined = sales_df.join(broadcast(customer_df), on=sales_df.Customer_ID==customer_df.Customer_ID, how=\"left_outer\")\n",
    "    df_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c67e3ee-af9a-4bf9-9363-48e12382cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for benchmarking\n",
    "from pyspark.sql.functions import count, lit , broadcast\n",
    "@get_time\n",
    "def x(): \n",
    "    df_joined = spark.sql(\"\"\"SELECT /*+ BROADCAST(customer) */\n",
    "                            sales.*,\n",
    "                            customer.*\n",
    "                            FROM sales\n",
    "                            LEFT OUTER JOIN customer\n",
    "                            ON sales.Customer_ID = customer.Customer_ID\n",
    "                                \"\"\")\n",
    "    df_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccecb7c-5dc0-4c04-92c5-4dcb6cb902e6",
   "metadata": {},
   "source": [
    "##Big table vs Big Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32cf00a6-62ad-42e5-a31a-013161039f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_schema = \"transacted_at string, trx_id string, retailer_id string, description string, amount double, city_id string\"\n",
    "\n",
    "sales = spark.read.format(\"csv\").schema(sales_schema).option(\"header\", True).load(\"./Input/new_sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "320efe92-0dee-4050-a76a-a07687687b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read City data\n",
    "\n",
    "city_schema = \"city_id string, city string, state string, state_abv string, country string\"\n",
    "\n",
    "city = spark.read.format(\"csv\").schema(city_schema).option(\"header\", True).load(\"./Input/cities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a24f36da-a50a-426e-89ee-f241d423f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Data\n",
    "\n",
    "df_sales_joined = sales.join(city, on=sales.city_id==city.city_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92ac98d6-c716-4efb-881c-97a62ff71dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Execution time: 30005.728721618652 ms\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Code for benchmarking\n",
    "from pyspark.sql.functions import count, lit \n",
    "@get_time\n",
    "def x(): \n",
    "    df_sales_joined.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c194ec98-b665-42a1-92fb-18723ad944e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Sales data in Buckets\n",
    "\n",
    "sales.write.format(\"csv\").mode(\"overwrite\").bucketBy(4, \"city_id\").option(\"header\", True).saveAsTable(\"sales_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bba4b4ea-f00f-41d6-91b0-84f49eda344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write City data in Buckets\n",
    "\n",
    "city.write.format(\"csv\").mode(\"overwrite\").bucketBy(4, \"city_id\").option(\"header\", True).saveAsTable(\"city_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b56bf8f-19b5-45c2-86e0-bf1e18be1377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+\n",
      "|namespace|   tableName|isTemporary|\n",
      "+---------+------------+-----------+\n",
      "|  default| city_bucket|      false|\n",
      "|  default|sales_bucket|      false|\n",
      "+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check tables\n",
    "\n",
    "spark.sql(\"show tables in default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "853f8243-806e-4d13-86f6-444ce77a6f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Sales table\n",
    "\n",
    "sales_bucket = spark.read.table(\"sales_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e72a7a73-f619-4ceb-a482-94fba8309c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read City table\n",
    "\n",
    "city_bucket = spark.read.table(\"city_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615ce503-f1c5-481f-8e53-bbd49d1f9d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a sample of user_id distribution in users table\n",
    "sales_bucket.select(\"city_id\").distinct().show()\n",
    "\n",
    "# Show a sample of user_id distribution in orders table\n",
    "city_bucket.select(\"city_id\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "540c4cda-14a7-482b-a3c0-3fa94ce54799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+---------+-------+\n",
      "|   city_id|      city|state|state_abv|country|\n",
      "+----------+----------+-----+---------+-------+\n",
      "|1030993386|Montevideo| NULL|     NULL|Uruguay|\n",
      "+----------+----------+-----+---------+-------+\n",
      "\n",
      "+--------------------+----------+-----------+--------------------+-------+----------+\n",
      "|       transacted_at|    trx_id|retailer_id|         description| amount|   city_id|\n",
      "+--------------------+----------+-----------+--------------------+-------+----------+\n",
      "|2017-12-09T12:00:...|1614079909|  847200066|            Wal-Mart|   5.62|1030993386|\n",
      "|2017-12-24T22:00:...| 745524396|  386167994|Wendy's          ...|  324.3|1030993386|\n",
      "|2017-12-14T23:00:...|1080477925| 1295306792|Dick's Sporting G...|  84.38|1030993386|\n",
      "|2017-11-25T23:00:...|1729862678|  847200066|Wal-Mart    arc i...|   47.6|1030993386|\n",
      "|2017-12-09T12:00:...|1614145382|  902350112|DineEquity   ccd ...|2796.67|1030993386|\n",
      "|2017-01-15T19:00:...|1344235985| 1334799521|TJ Max    ppd id:...|2590.72|1030993386|\n",
      "|2017-12-03T20:00:...|1533968784|    9225731|unkn     arc id: ...| 280.62|1030993386|\n",
      "|2017-12-24T22:00:...| 855397514|  529458728|unkn     arc id: ...|   2.88|1030993386|\n",
      "|2017-12-15T18:00:...|1471186503|  529458728|unkn        Monte...|  35.87|1030993386|\n",
      "|2017-01-15T19:00:...|1278293975|  683159064|Safeway     ppd i...|  128.2|1030993386|\n",
      "|2017-12-20T20:00:...| 243252249|  847200066|unkn  ppd id: 999...|   14.4|1030993386|\n",
      "|2017-11-24T06:00:...|1943632325| 2077350195|Walgreen     ccd ...|2846.15|1030993386|\n",
      "|2017-12-15T18:00:...|1633120955|  103953879|  Rite Aid     12-16|  39.15|1030993386|\n",
      "|2017-01-15T19:00:...|1278240843| 1953761884|Home Depot     ar...|  659.6|1030993386|\n",
      "|2017-12-29T23:00:...| 436775533|  799199962|PetSmart   ccd id...| 230.65|1030993386|\n",
      "|2017-12-29T22:00:...| 237134402| 2001148981|Costco   arc id: ...|  34.99|1030993386|\n",
      "|2017-12-15T18:00:...|1633296046| 1654681099|unkn     ccd id: ...| 1459.9|1030993386|\n",
      "|2017-01-15T19:00:...|1278208202| 2092104004|SUPERVALU  ppd id...| 251.17|1030993386|\n",
      "|2017-12-29T23:00:...| 650403628| 2139149619|Trader Joe's  arc...| 297.38|1030993386|\n",
      "|2017-11-24T06:00:...|1943686950|  683159064|unkn    arc id: 5...|  45.61|1030993386|\n",
      "+--------------------+----------+-----------+--------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# Assuming that both tables are bucketed into 4 buckets (0 to 3)\n",
    "# Filter users for bucket 0\n",
    "city_bucket_0 = city_bucket.filter((F.hash(\"city_id\") % 4) == 0)\n",
    "\n",
    "# Filter orders for bucket 0\n",
    "sales_bucket_0 = sales_bucket.filter((F.hash(\"city_id\") % 4) == 0)\n",
    "\n",
    "# Show the content of bucket 0 for both tables\n",
    "city_bucket_0.where(\"city_id = '1030993386'\").show()\n",
    "sales_bucket_0.orderBy(\"city_id\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5387b145-9937-495a-bfac-3bf288febccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join datasets\n",
    "\n",
    "df_joined_bucket = sales_bucket.join(city_bucket, on=sales_bucket.city_id==city_bucket.city_id, how=\"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7db1d0c6-6c83-4f64-95f0-b1f0280a44c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Execution time: 19095.71671485901 ms\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Write dataset\n",
    "# Code for benchmarking\n",
    "from pyspark.sql.functions import count, lit \n",
    "@get_time\n",
    "def x(): \n",
    "    df_joined_bucket.write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bff10941-dae0-43dd-9a7b-d3ef0ba18f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61bc5517-13a2-4b6c-b00b-f701f2c7ec72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [city_id#289], [city_id#296], LeftOuter\n",
      "   :- Sort [city_id#289 ASC NULLS FIRST], false, 0\n",
      "   :  +- FileScan csv spark_catalog.default.sales_bucket[transacted_at#284,trx_id#285,retailer_id#286,description#287,amount#288,city_id#289] Batched: false, Bucketed: true, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jupyter/PySpark Tutorials - Qbex/07 - Performance Tuning/sp..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transacted_at:string,trx_id:string,retailer_id:string,description:string,amount:double,cit..., SelectedBucketsCount: 4 out of 4\n",
      "   +- Sort [city_id#296 ASC NULLS FIRST], false, 0\n",
      "      +- Filter isnotnull(city_id#296)\n",
      "         +- FileScan csv spark_catalog.default.city_bucket[city_id#296,city#297,state#298,state_abv#299,country#300] Batched: false, Bucketed: true, DataFilters: [isnotnull(city_id#296)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jupyter/PySpark Tutorials - Qbex/07 - Performance Tuning/sp..., PartitionFilters: [], PushedFilters: [IsNotNull(city_id)], ReadSchema: struct<city_id:string,city:string,state:string,state_abv:string,country:string>, SelectedBucketsCount: 4 out of 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined_bucket.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141993f5-9419-4cab-9300-09023f3cd8f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
